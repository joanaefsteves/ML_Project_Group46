{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqbFmKvxoroF"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## <center> GROUP PROJECT - TO GRANT OR NOT TO GRANT: DECIDING ON COMPENSATION BENEFITS </center> <br>\n",
    "#  <center> <b> PREPROCESS </center> <br>\n",
    "## <center> Fall Semester 2024-2025 <center>\n",
    "<br>\n",
    "<center> Group 46: <center> <br>\n",
    "<center>Afonso Ascensão, 20240684 <br><center>\n",
    "<center>Duarte Marques, 20240522 <br><center>\n",
    "<center>Joana Esteves, 20240746 <br><center>\n",
    "<center>Rita Serra, 20240515 <br><center>\n",
    "<center>Rodrigo Luís, 20240742 <br><center>\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description of notebook contents:**\n",
    "- Apply tranformations that could be implemented before the split, namely: dropping variables selected during EDA for being problematic, correction of data types, correction of incongruencies and part of the feature engineering.\n",
    "- Perform a split that will be used for all model's notebooks. Generate files for train and validation data before and after split to use as needed for the models.\n",
    "- Implement a pipeline to do after split tranformations inlcluding: outlier handeling, missing values inputation, feature engineering, categoric variables cardinality reduction, encoding, scaling and performing feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMXFdXdzoroI"
   },
   "source": [
    "**Table of Contents**\n",
    "- [1. Import the needed Libraries](#importlibraries)\n",
    "- [2. Import Dataset](#importdataset)\n",
    "- [3. Preprocessing](#section_3)\n",
    "    - [3.1. Variables to drop](#section_3_1)\n",
    "    - [3.2. Data Types](#section_3_2)\n",
    "    - [3.3. Strange Values](#section_3_3)\n",
    "    - [3.4. Feature Engineering](#section_3_4)\n",
    "    - [3.5. Split](#section_3_5)\n",
    "    - [3.6. Pipeline](#section_3_6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BL9oVLaT6fER"
   },
   "source": [
    "<a class=\"anchor\" id=\"importlibraries\">\n",
    "\n",
    "# 1. Import the needed Libraries\n",
    "\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "69HiGJ-s8roW",
    "outputId": "80b74294-f5eb-4daa-d490-0fc7f7897f8b"
   },
   "outputs": [],
   "source": [
    "#%pip install pyarrow\n",
    "#%pip install joblib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "## Export pipeline\n",
    "import joblib\n",
    "## py file with custom tranformer for pipeline\n",
    "from transformers import *\n",
    "\n",
    "# Display all rows \n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WVnubShx6mP0"
   },
   "source": [
    "<a class=\"anchor\" id=\"importdataset\">\n",
    "\n",
    "# 2. Import Dataset\n",
    "\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rcouc\\AppData\\Local\\Temp\\ipykernel_5048\\1422952055.py:1: DtypeWarning: Columns (29) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_data = pd.read_csv('train_data.csv', sep = ',')\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('train_data.csv', sep = ',')\n",
    "test_data = pd.read_csv('test_data.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"section_3\">\n",
    "\n",
    "# 3. Preprocessing\n",
    "\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"section_3_1\">\n",
    "\n",
    "## 3.1. Variables to drop\n",
    "\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to our EDA this variables have a high percentage of nan and are redudant with other variables, so we will drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable with 100% nan\n",
    "train_data.drop([\"OIICS Nature of Injury Description\"], axis=1, inplace=True)\n",
    "test_data.drop([\"OIICS Nature of Injury Description\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This variable is 5% nan and is redundant with \"Medical Fee Region\" (Cramer's V Matrix shows correlation of 1), \n",
    "# and highly correlated with other location variables.\n",
    "train_data.drop([\"Zip Code\"], axis=1, inplace=True)\n",
    "test_data.drop([\"Zip Code\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correspondent codes and descriptions have a correlation of 1 and as such the information is redundant\n",
    "train_data = train_data.drop(columns=['WCIO Cause of Injury Description','WCIO Nature of Injury Description','WCIO Part Of Body Description','Industry Code Description'])\n",
    "test_data = test_data.drop(columns=['WCIO Cause of Injury Description','WCIO Nature of Injury Description','WCIO Part Of Body Description','Industry Code Description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Birth Year is redudant with age at injury and is 9% nan, we will keep it to input nan values and after that we will remove it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also drop the variables in the train dataset that won't be used as features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop(['WCB Decision'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zy8zRKKTgffC"
   },
   "source": [
    "<a class=\"anchor\" id=\"section_3_2\">\n",
    "\n",
    "## 3.2. Data Types\n",
    "\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xz9JWK2zoroU",
    "outputId": "9fbf5f88-a4b1-4d06-ddc1-f01a37421fcd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rcouc\\AppData\\Local\\Temp\\ipykernel_5048\\1867619682.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  train_data[code_columns] = train_data[code_columns].applymap(lambda x: str(x) if pd.notna(x) else x)\n",
      "C:\\Users\\rcouc\\AppData\\Local\\Temp\\ipykernel_5048\\1867619682.py:7: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  test_data[code_columns] = test_data[code_columns].applymap(lambda x: str(x) if pd.notna(x) else x)\n"
     ]
    }
   ],
   "source": [
    "# Convert variables related to codes to strings\n",
    "code_columns = [\"Industry Code\", \"WCIO Cause of Injury Code\",\n",
    "                \"WCIO Nature of Injury Code\", \"WCIO Part Of Body Code\"]\n",
    "\n",
    "train_data[code_columns] = train_data[code_columns].applymap(lambda x: str(x) if pd.notna(x) else x)\n",
    "\n",
    "test_data[code_columns] = test_data[code_columns].applymap(lambda x: str(x) if pd.notna(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "DIt8KJDloroV"
   },
   "outputs": [],
   "source": [
    "#Convert some columns to date format as it's more suitable\n",
    "\n",
    "date_columns = ['Accident Date', 'C-2 Date', 'C-3 Date', 'Assembly Date', 'First Hearing Date']\n",
    "\n",
    "for column in date_columns:\n",
    "    train_data[column] = pd.to_datetime(train_data[column], errors='coerce')\n",
    "    test_data[column] = pd.to_datetime(test_data[column], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "yLR8xmx6oroV"
   },
   "outputs": [],
   "source": [
    "# Convert some columns to integer format as it's more suitable\n",
    "# Use int64 to keep nan for now\n",
    "\n",
    "int_columns = ['Age at Injury', 'IME-4 Count', 'Number of Dependents']\n",
    "\n",
    "for column in int_columns:\n",
    "    train_data[column] = train_data[column].astype('Int64')\n",
    "    test_data[column] = test_data[column].astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert \"Attorney/Representative\" to boolean\n",
    "train_data[\"Attorney/Representative\"] = train_data[\"Attorney/Representative\"].map({\"Y\": True, \"N\": False}).astype(bool)\n",
    "test_data[\"Attorney/Representative\"] = test_data[\"Attorney/Representative\"].map({\"Y\": True, \"N\": False}).astype(bool)\n",
    "\n",
    "# Convert \"COVID-19 Indicator\" to boolean\n",
    "train_data[\"COVID-19 Indicator\"] = train_data[\"COVID-19 Indicator\"].map({\"Y\": True, \"N\": False}).astype(bool)\n",
    "test_data[\"COVID-19 Indicator\"] = test_data[\"COVID-19 Indicator\"].map({\"Y\": True, \"N\": False}).astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFzpmY9KQPDs"
   },
   "source": [
    "<a class=\"anchor\" id=\"section_3_3\">\n",
    "\n",
    "## 3.3. Strange values\n",
    "\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our EDA we concluded:\n",
    "- There are invalid dates, where Assemby Date is earlier than Accident Date, we will correct this cases.\n",
    "- All 9 digit claim id lines have missing values for all varibles except \"Assembly Date\", so we will remove this lines.\n",
    "- Numeric features: There are zero values that can be interpreted as missing values and need imputation - namely Age at injury, Average weekly wage and Birth year.\n",
    "- Categoric features: There are some variables we believe should be binary and will consider categories like U (unkown) or X (possibly non-binary - very low frequency) as missing values for future inputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "CcXr9MS3oroX"
   },
   "outputs": [],
   "source": [
    "# For the detected cases change accident date to match assembly date\n",
    "train_data.loc[(train_data['Assembly Date'] < train_data['Accident Date']), \"Accident Date\"] = train_data['Assembly Date']\n",
    "test_data.loc[(test_data['Assembly Date'] < test_data['Accident Date']), \"Accident Date\"] = test_data['Assembly Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all lines with a 9 digit clain identifier\n",
    "nine_digit_id = train_data.loc[train_data[\"Claim Identifier\"] > 10_000_000]\n",
    "\n",
    "# Remove 9 digit claim id lines from the dataset\n",
    "train_data = train_data.drop(nine_digit_id.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the 0 values with nan so it's easier to correct them \n",
    "\n",
    "train_data['Average Weekly Wage'] = train_data['Average Weekly Wage'].replace(0, np.nan)\n",
    "test_data['Average Weekly Wage'] = test_data['Average Weekly Wage'].replace(0, np.nan)\n",
    "\n",
    "train_data['Birth Year'] = train_data['Birth Year'].replace(0, np.nan)\n",
    "test_data['Birth Year'] = test_data['Birth Year'].replace(0, np.nan)\n",
    "\n",
    "train_data['Age at Injury'] = train_data['Age at Injury'].replace(0, np.nan)\n",
    "test_data['Age at Injury'] = test_data['Age at Injury'].replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing U and X categories with nan so it's easier to correct them\n",
    "\n",
    "train_data['Gender'] = train_data['Gender'].replace([\"U\", \"X\"], np.nan).map({'M': True, 'F': False}).astype(bool)\n",
    "test_data['Gender'] = test_data['Gender'].replace([\"U\", \"X\"], np.nan).map({'M': True, 'F': False}).astype(bool)\n",
    "\n",
    "train_data['Alternative Dispute Resolution'] = train_data['Alternative Dispute Resolution'].replace(\"U\", np.nan).map({\"Y\": True, \"N\": False}).astype(bool)\n",
    "test_data['Alternative Dispute Resolution'] = test_data['Alternative Dispute Resolution'].replace(\"U\", np.nan).map({\"Y\": True, \"N\": False}).astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"section_3_4\">\n",
    "\n",
    "## 3.4. Feature Engineering\n",
    "\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binary variables:**\n",
    "- \"Hearing Held\": Indicates if an hearing was held or not.\n",
    "- \"C-2 Delivered\": Indicates if this form was ever delivered. \n",
    "- \"C-2 Delivered on Time\": Indicates in the C-2 form was delivered in the established time (up to 10 days).\n",
    "- \"C-3 Delivered\": Indicates if this form as ever delivered.\n",
    "- \"C-3 Delivered on Time\" : Indicates if the form was delivered in the established time (up to 730 days).\n",
    "\n",
    "**Mathematical tranformations:**\n",
    "- \"Average Weekly Wage Log\": After zero removal (for the plot) we saw that Average Weekly Wage was right skewed, we will create a new feature with a log tranformation. \n",
    "- \"IME-4 Count log\": IME-4 Count is right skewed, we will create a new feature with a log transformation.\n",
    "\n",
    "**Temporal lags:**\n",
    "- \"Time Accident to Assembly\": Days between accident date and assembly date.\n",
    "- \"Time Assembly to Hearing\": Days between assembly date and first hearing date.\n",
    "\n",
    "**Dates to Year:**\n",
    "- \"Accident Year\": There year when the accident happend. \n",
    "- \"Assembly Year\": The year when the claim was assembled.\n",
    "\n",
    "**Union of binary variables:**\n",
    "- 'At/rp OR altr dispute': Logical OR for Alternative Dispute Resolution and Attorney/Representative.\n",
    "- 'Covid OR altr dispute': Logical OR for Covid 19 Indicator and Attorney/Representative.\n",
    "- 'Covid OR At/rp': Logical OR for Covid 19 Indicator and Attorney/Representative.\n",
    "\n",
    "**Ratios:**\n",
    "- 'Wage Age Ratio': Ratio between the average weekly wage (after log) and the age.\n",
    "\n",
    "**Extreme outlier flags:**\n",
    "- 'High Wage Flag': Flags values in top 5% for the average weekly wage.\n",
    "- 'High count IME-4 Flag': Flags values in top 1% for the number of IME-4 forms delivered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dates to Year:\n",
    "\n",
    "train_data['Accident Year'] = train_data['Accident Date'].dt.year\n",
    "test_data['Accident Year'] = test_data['Accident Date'].dt.year\n",
    "\n",
    "train_data['Assembly Year'] = train_data['Assembly Date'].dt.year\n",
    "test_data['Assembly Year'] = test_data['Assembly Date'].dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The remaining feature will be computed in the pipeline after outliers and missing values handeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"section_3_5\">\n",
    "\n",
    "## 3.5. Split\n",
    "\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.set_index(\"Claim Identifier\")\n",
    "test_data = test_data.set_index(\"Claim Identifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset with before split tranformations (tranformations from 3.1 to 3.4) to use for CV in the models notebooks,\n",
    "# where we will use the pipeline to complete the preprocessing\n",
    "\n",
    "# Use parquet to perserve data types\n",
    "train_data.to_parquet(\"transformed_train_data.parquet\", index=True)\n",
    "test_data.to_parquet(\"transformed_test_data.parquet\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data.drop(['Claim Injury Type'], axis = 1)\n",
    "y = train_data['Claim Injury Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified split to deal with classe unbalance in the target\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y, test_size = 0.1,\n",
    "                                                  random_state = 0,\n",
    "                                                  stratify = y,\n",
    "                                                  shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train and validation data after split with before split tranformation to use for the models notebooks,\n",
    "# where we will apply the pipeline to complete preprocessing\n",
    "transformed_train_split = pd.concat([pd.DataFrame(X_train), pd.Series(y_train, name=\"Claim Injury Type\")], axis=1)\n",
    "transformed_val_split = pd.concat([pd.DataFrame(X_val), pd.Series(y_val, name=\"Claim Injury Type\")], axis=1)\n",
    "\n",
    "transformed_train_split.to_parquet(\"transformed_train_split.parquet\", index=True)\n",
    "transformed_val_split.to_parquet(\"transformed_val_split.parquet\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"section_3_6\">\n",
    "\n",
    "## 3.6. Pipeline\n",
    "\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pipeline.joblib']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing pipeline\n",
    "pipeline = Pipeline([   \n",
    "    ('impute', ImputeTransformer()),  # Imputation of missing values\n",
    "    ('outliers', OutlierHandlingTransformer()),  # Outlier handling\n",
    "    ('feature_engineering', FeatureEngineeringTransformer()), # Performs feature engineering \n",
    "    ('cardinality', HighCardinalityTransformer()), # Reduce classes of high cardinality features\n",
    "    ('encoding', EncodingTransformer()),  # Encoding categorical variables\n",
    "    ('scaling', ScalingTransformer()),  # Scaling numeric features\n",
    "    ('feature_selection', FeatureSelectionTransformer()) # Selection features for models \n",
    "])\n",
    "\n",
    "# Save pipeline\n",
    "joblib.dump(pipeline, 'pipeline.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" # Testing \\nX_train.drop(['Agreement Reached'], axis=1, inplace=True)\\nX_val.drop(['Agreement Reached'], axis=1, inplace=True)\\n\\nX_train_preprocessed = pipeline.fit_transform(X_train)\\nX_val_preprocessed = pipeline.transform(X_val)\\ntest_data_preprocessed = pipeline.transform(test_data) \\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' # Testing \n",
    "X_train.drop(['Agreement Reached'], axis=1, inplace=True)\n",
    "X_val.drop(['Agreement Reached'], axis=1, inplace=True)\n",
    "\n",
    "X_train_preprocessed = pipeline.fit_transform(X_train)\n",
    "X_val_preprocessed = pipeline.transform(X_val)\n",
    "test_data_preprocessed = pipeline.transform(test_data) \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "MqbFmKvxoroF",
    "BL9oVLaT6fER",
    "WVnubShx6mP0",
    "Al0mdDUBoroL",
    "6JjINtuHoroN",
    "akZH_ydMoroO",
    "hcJOdUk1K1kz",
    "6bzFiE2BoroR",
    "XAVSrcx5oroU",
    "TpNi7Q5ioroU",
    "c3wplzqvoroU",
    "wBA2IAMcoroV",
    "FFzpmY9KQPDs",
    "QILO2RwcAxwV",
    "qJDNJDf4BUYd",
    "HMW-CmnSoroX",
    "kkH3JUYYE4VW",
    "ADZcJgYioroX",
    "nZeA99aHGzTQ",
    "z9PEn_R5NjdK"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
